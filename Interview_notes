# interview notes

模型压缩的三大方法：剪枝、蒸馏、量化

### 防止过拟合的方法

1.从模型方面

- model ensemble（或者叫 bagging） 多个模型融合，这是打比赛刷榜的一种常规方法。当然我还见过一些人用级联的模型embedding，如一级、二级级联。二级把眼睛/眉毛crop出来单独用眼睛、眉毛子网络。
- 随机Dropout， 让神经元的一些节点随机失活。
- 加L1/L2正则化。 通过约束参数的范数使其不要偏差太大


2.从数据方面

- Batch Normalization 归一化数据样本。 提到的对数据做 normalization 预处理, 使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 
但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生.

- 数据增广。最高效的就是扩大数据集的样本个数，但在实际应用中却比较难做到，如何在打比赛的时候，数据集往往是固定的不能用额外的数据集。因此在CV领域就出现了一些非常常见的数据增广的方式：
  - flip 把照片镜像翻转
  - rotation 扭转
  - resize 变形成不同的大小
  - color_random 加上一个α、β系数，改变图片的颜色
  - erode、dilate 腐蚀、扩大边界。这两个都是cv2提供API直接可以调用的。
  - crop 
 
这些是我在训练戴口罩的人脸关键点比赛时候训练baseline用到的数据增广的方法。


